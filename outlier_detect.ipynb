{
 "metadata": {
  "name": "outlier_detect"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "A quick outlier detection"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Welcome to pylab, a matplotlib-based Python environment [backend: module://IPython.kernel.zmq.pylab.backend_inline].\n",
        "For more information, type 'help(pylab)'.\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy.stats as sst\n",
      "import matplotlib.pylab as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Algorithm\n",
      "\n",
      "1. select h observations as inliers\n",
      "2. compute mean (mu) and variance (var)\n",
      "3. compute Malhanobis distance (Md) for all observations, using mu and var\n",
      "4. take the h observ that have smallest Md\n",
      "5. Repeat 2, 3, 4 until\n",
      "6. Stop criteria: compare with previous var and mu\n",
      "\n",
      "Return outlier corrected mu and var"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_mu_var(y):\n",
      "    \"\"\" Compute mean and (co)variance for one or more measures\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    y : (N,) or (P, N) ndarray\n",
      "        One row per measure, one column per observation. If a vector, treat as a\n",
      "        (1, N) array\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    mu : (P,) ndarray\n",
      "        Mean of each measure across columns.\n",
      "    var : (P, P) ndarray\n",
      "        Variances (diagonal) and covariances of measures\n",
      "    \"\"\"\n",
      "    # Make sure a vector becumes a 1, N 2D array\n",
      "    y = np.atleast_2d(y)\n",
      "    # Degrees of freedom\n",
      "    P, N = y.shape\n",
      "    df = N - 1\n",
      "    # Mean for each row\n",
      "    mu = y.mean(axis=1)\n",
      "    # The mean removed the second axis. Restore it (length 1) so we can subtract\n",
      "    subtracting_mu = np.reshape(mu, (P, 1))\n",
      "    # Remove mean\n",
      "    yc = y - subtracting_mu\n",
      "    # Variance(s) and covariances\n",
      "    var = yc.dot(yc.T) / df\n",
      "    return mu, var"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test the mu / var function\n",
      "assert compute_mu_var(np.asarray([[1, 1, 1, 1]])) == (1,0)\n",
      "assert compute_mu_var(np.asarray([[-1, 0, 1]])) == (0,1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_mahal(y, mu, var):\n",
      "    \"\"\" Compute Mahalanobis distance for `y` given `mu`, `var`\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    y : (N,) or (P, N) ndarray\n",
      "        One row per measure, one column per observation. If a vector, treat as a\n",
      "        (1, N) array\n",
      "    mu : (P,) array-like\n",
      "        Mean of each measure across columns.  Can be scalar, array or sequence\n",
      "        (list, tuple)\n",
      "    var : (P, P) array-like\n",
      "        Variances (diagonal) and covariances of measures. Can be scalar, array\n",
      "        or sequence (list, tuple)\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    mahals : (N,) ndarray\n",
      "        Mahalanobis distances of each observation from `mu`, given `var`\n",
      "    \"\"\"\n",
      "    # Make sure y is a row vector, if it was only a 1D vector\n",
      "    y = np.atleast_2d(y)\n",
      "    # Shapes\n",
      "    P, N = y.shape\n",
      "    # Make sure mu and var are arrays\n",
      "    mu = np.asarray(mu)\n",
      "    # Variance should also be 2D (even if shape (1, 1)) - for np.linalg.inv\n",
      "    var = np.atleast_2d(var)\n",
      "    # The mean should be shape (P,).  It needs to be (P, 1) shape to subtract\n",
      "    subtracting_mu = np.reshape(mu, (P, 1))\n",
      "    # Mean correct\n",
      "    yc = y - subtracting_mu\n",
      "    # Correct for (co)variances. For single row, this is the same as dividing by\n",
      "    # the variance\n",
      "    y_white = np.linalg.inv(var).dot(yc)\n",
      "    # Z score for one row is (y - mu) / sqrt(var).\n",
      "    # Z**2 is (y - mu) (y-nu) / var, which is:\n",
      "    z2 = yc * y_white\n",
      "    # Mahalanobis distance is mean z2 over measures\n",
      "    return z2.mean(axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test the Mahalonbis function\n",
      "distances = compute_mahal([-1, 0, 1], 1, 1), \n",
      "assert np.allclose(distances, [ 4.,  1.,  0.])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def estimate_mu_var(y, n_inliers=0.7, maxiter=10, tol=1e-6):\n",
      "    \"\"\" Estimate corrected `mu` and `var` for `y` given number of inliers\n",
      "\n",
      "    Algorithm from:\n",
      "\n",
      "    Fritsch, V., Varoquaux, G., Thyreau, B., Poline, J. B., & Thirion, B. (2012).\n",
      "    Detecting outliers in high-dimensional neuroimaging datasets with robust\n",
      "    covariance estimators. Medical Image Analysis.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    y : (N,) or (P, N) ndarray\n",
      "        One row per measure, one column per observation. If a vector, treat as a\n",
      "        (1, N) array\n",
      "    n_inliers : int or float, optional\n",
      "        If int, the number H (H < N) of observations in the center of the\n",
      "        distributions that can be assumed to be non-outliers. If float, should\n",
      "        be between 0 and 1, and give proportion of inliers\n",
      "    maxiter : int, optional\n",
      "        Maximum number of iterations to refine estimate of outliers\n",
      "    tol : float, optional\n",
      "        Smallest change in variance estimate for which we contine to iterate.\n",
      "        Changes smaller than `tol` indicate that iteration of outlier detection\n",
      "        has converged\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    mu : (P,) ndarray\n",
      "        Mean per measure in `y`, correcting for possible outliers\n",
      "    var : (P, P) ndarray\n",
      "        Variances (diagonal) and covariances (off-diagonal) for measures `y`,\n",
      "        correcting for possible outliers\n",
      "    \"\"\"\n",
      "    y = np.atleast_2d(y)\n",
      "    P, N = y.shape\n",
      "    if n_inliers > 0 and n_inliers < 1: # Proportion of inliers\n",
      "        n_inliers = int(np.round(N * n_inliers))\n",
      "    if n_inliers <= 0:\n",
      "        raise ValueError('n_inliers should be > 0')\n",
      "    # Compute first estimate of mu and varances\n",
      "    mu, var = compute_mu_var(y)\n",
      "    if n_inliers >= N:\n",
      "        return mu, var\n",
      "    # Initialize estimate of which are the inlier values.\n",
      "    prev_inlier_indices = np.arange(n_inliers)\n",
      "    # Keep pushing outliers to the end until we are done\n",
      "    for i in range(maxiter):\n",
      "        distances = compute_mahal(y, mu, var)\n",
      "        # Pick n_inliers observatons with lowest distances\n",
      "        inlier_indices = np.argsort(distances)[:n_inliers]\n",
      "        # If we found the same inliers as last time, we'll get the same mu, var,\n",
      "        # so stop iterating\n",
      "        if np.all(inlier_indices == prev_inlier_indices):\n",
      "            break\n",
      "        # Re-estimate mu and var with new inliers\n",
      "        mu_new, var_new = compute_mu_var(y[:, inlier_indices])\n",
      "        # Check if var has changed - if not - stop iterating\n",
      "        if np.max(np.abs(var - var_new)) < tol:\n",
      "            break\n",
      "        # Set mu, var, indices for next loop iteration\n",
      "        var = var_new\n",
      "        mu = mu_new\n",
      "        prev_inlier_indices = inlier_indices\n",
      "    return mu, var"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Do a run through of the outlier detection\n",
      "# Number of samples\n",
      "n_samples = 100\n",
      "# Standard deviation\n",
      "sigma = 1.\n",
      "# Data vectors (no outliers yet)\n",
      "Y = np.random.normal(0., sigma, size=(1, n_samples))\n",
      "# Proportion of inliers (1-<proportion of outliers>\n",
      "inlier_proportion = .70\n",
      "# Number of inliers, outliers\n",
      "n_inliers = int(inlier_proportion * n_samples)\n",
      "n_outliers = n_samples - n_inliers\n",
      "# Make the correct number of outliers\n",
      "outliers = np.random.normal(0., sigma*10, size=(1, n_outliers))\n",
      "Y[:, 0:n_outliers] = outliers\n",
      "\n",
      "# Estimate the outlier corrected mean and variance\n",
      "print('Uncorrected mu and var')\n",
      "print(compute_mu_var(Y))\n",
      "corr_mu, corr_var = estimate_mu_var(Y)\n",
      "print('Corrected mu and var')\n",
      "print(corr_mu, corr_var)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Uncorrected mu and var\n",
        "(array([-0.01926975]), array([[ 27.22528075]]))\n",
        "Corrected mu and var\n",
        "(array([ 0.1059014]), array([[ 0.76090062]]))\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Detection step"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# choose a false positive rate - Bonferroni corrected for number of samples\n",
      "alpha = 0.1 / n_samples\n",
      "\n",
      "# Standard deviation - for our 1D case\n",
      "corr_sigma = np.sqrt(corr_var)\n",
      "# z - in 1D case\n",
      "z = (Y - corr_mu) / corr_sigma\n",
      "\n",
      "_out = plt.hist(z[0,:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD9CAYAAAChtfywAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBwDDEeQAYDiCHAAMR5ADgOEIcgAw3P8BqRP1H9fC5QcAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1069cdb90>"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Normal distribution object\n",
      "normdist = sst.norm(corr_mu, corr_sigma)\n",
      "# Probability of z value less than or equal to each observation\n",
      "p_values = normdist.cdf(z)\n",
      "# Where probability too high therefore z value too large.  We're only looking\n",
      "# for z's that are too positive, disregarding zs that are too negative\n",
      "some_bad_ones = p_values > (1 - alpha)\n",
      "\n",
      "# Show what we found\n",
      "print \"volumes to remove :\", some_bad_ones\n",
      "print z[some_bad_ones]\n",
      "print Y[some_bad_ones]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "volumes to remove : [[False False  True  True False  True False False  True False False  True\n",
        "   True False False  True  True  True  True False False  True False False\n",
        "  False False False False  True  True False False False False False False\n",
        "  False False False False False False False False False False False False\n",
        "  False False False False False False False False False False False False\n",
        "  False False False False False False False False False False False False\n",
        "  False False False False False False False False False False False False\n",
        "  False False False False False False False False False False False False\n",
        "  False False False False]]\n",
        "[ 22.08761142   6.2333577   22.16160714   8.39662933   7.5859097\n",
        "   9.87665987   9.39303528   3.58721626   8.49534474   5.78112211\n",
        "   6.16594538   3.36706642   3.24341882]\n",
        "[ 19.37284035   5.54323547  19.43738654   7.43024905   6.72306142\n",
        "   8.72127403   8.29941015   3.23501642   7.51635812   5.14875211\n",
        "   5.48443197   3.04298056   2.93512323]\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# exercise : \n",
      "# print histogram of the good ones:"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# scrap cell\n",
      "#========================================#\n",
      "\n",
      "# here - just one variable\n",
      "#z0 = z[0,:]\n",
      "#mu0 = mu[0]\n",
      "#sig0 = sig[0,0]\n",
      "#print good_ones\n",
      "#good_ones = np.where(some_bad_ones == False)\n",
      "#print good_ones.shape\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    }
   ],
   "metadata": {}
  }
 ]
}